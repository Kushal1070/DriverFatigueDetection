{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Driver drowsiness detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the libraries required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Organizing the paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting main path\n",
    "\n",
    "master_path = \"data_eye\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and test path\n",
    "\n",
    "train_path = os.path.join(master_path, \"train\")\n",
    "test_path = os.path.join(master_path, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add test data randomly from the train data sets\n",
    "\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "# source\n",
    "open_train_path = os.path.join(train_path, \"open_eye\")\n",
    "close_train_path = os.path.join(train_path, \"closed_eye\")\n",
    "\n",
    "# destination\n",
    "open_test_path = os.path.join(test_path, \"open_eye\")\n",
    "close_test_path = os.path.join(test_path, \"closed_eye\")\n",
    "\n",
    "source1 = open_train_path\n",
    "destination1 = open_test_path\n",
    "files1 = os.listdir(source1)\n",
    "no_of_files1 = len(files1) // 5\n",
    "\n",
    "# for file_name1 in random.sample(files1, no_of_files1):\n",
    "#     shutil.move(os.path.join(source1, file_name1), destination1)\n",
    "\n",
    "source2 = close_train_path\n",
    "destination2 = close_test_path\n",
    "files2 = os.listdir(source2)\n",
    "no_of_files2 = len(files2) // 5\n",
    "\n",
    "# print(no_of_files1, no_of_files2)\n",
    "\n",
    "# for file_name2 in random.sample(files2, no_of_files2):\n",
    "#     shutil.move(os.path.join(source2, file_name2), destination2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# closed and open eye image paths\n",
    "\n",
    "open_train_images = glob.glob(train_path+\"\\\\open_eye/*.png\")\n",
    "closed_train_images = glob.glob(train_path+\"\\\\closed_eye/*.png\")\n",
    "\n",
    "open_test_images = glob.glob(test_path+\"\\\\open_eye/*.png\")\n",
    "closed_test_images = glob.glob(test_path+\"\\\\closed_eye/*.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating training and test dataframes\n",
    "\n",
    "train_list = [x for x in closed_train_images]\n",
    "train_list.extend([x for x in open_train_images])\n",
    "\n",
    "df_train = pd.DataFrame(np.concatenate([[\"closed_eye\"]*len(closed_train_images),\n",
    "                                        [\"open_eye\"]*len(open_train_images)]), columns=[\"class\"])\n",
    "df_train[\"image\"] = [x for x in train_list]\n",
    "\n",
    "test_list = [x for x in closed_test_images]\n",
    "test_list.extend([x for x in open_test_images])\n",
    "\n",
    "df_test = pd.DataFrame(np.concatenate([[\"closed_eye\"]*len(closed_test_images),\n",
    "                                       [\"open_eye\"]*len(open_test_images)]), columns=[\"class\"])\n",
    "df_test[\"image\"] = [x for x in test_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.title(\"Number of cases\", fontsize=12)\n",
    "sns.countplot(data=df_train, x=\"class\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.title(\"Number of cases\", fontsize=12)\n",
    "sns.countplot(data=df_test, x=\"class\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eye images\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=6, figsize=(15,10))\n",
    "\n",
    "for i , ax in enumerate(axes.flat):\n",
    "    img = cv2.imread(closed_train_images[i])\n",
    "    img = cv2.resize(img, (512,512))\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(\"Closed\")\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eye images\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=6, figsize=(15,10))\n",
    "\n",
    "for i , ax in enumerate(axes.flat):\n",
    "    img = cv2.imread(open_train_images[i])\n",
    "    img = cv2.resize(img, (512,512))\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(\"Open\")\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing and preparing the dataset\n",
    "\n",
    "#Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, val_df = train_test_split(df_train, test_size=0.2, random_state=13, stratify=df_train[\"class\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1/255)\n",
    "\n",
    "val_datagen = ImageDataGenerator(rescale=1/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "                  train_df,\n",
    "                  x_col=\"image\",\n",
    "                  y_col=\"class\",\n",
    "                  target_size=(150,150),\n",
    "                  batch_size=32,\n",
    "                  class_mode=\"binary\",\n",
    "                  seed=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_generator = val_datagen.flow_from_dataframe(\n",
    "                val_df,\n",
    "                x_col=\"image\",\n",
    "                y_col=\"class\",\n",
    "                target_size=(150,150),\n",
    "                batch_size=32,\n",
    "                class_mode=\"binary\",\n",
    "                seed=7)\n",
    "\n",
    "test_generator = val_datagen.flow_from_dataframe(\n",
    "                 df_test,\n",
    "                 x_col=\"image\",\n",
    "                 y_col=\"class\",\n",
    "                 target_size=(150,150),\n",
    "                 batch_size=32,\n",
    "                 class_mode=\"binary\",\n",
    "                 shuffle=False,\n",
    "                 seed=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the necessary packages\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, BatchNormalization\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#building the cnn\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "#convolution\n",
    "# TODO\n",
    "# Diagram\n",
    "model.add(Conv2D(filters=16, kernel_size=(3,3), activation=\"relu\", input_shape=(150,150,3)))\n",
    "\n",
    "#pooling\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "#2nd Conv\n",
    "model.add(Conv2D(filters=32, kernel_size=(3,3), activation=\"relu\"))\n",
    "\n",
    "#2nd pooling\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "#3rd Conv\n",
    "model.add(Conv2D(filters=64, kernel_size=(3,3), activation=\"relu\"))\n",
    "\n",
    "#3rd Pooling\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "#flatten\n",
    "model.add(Flatten())\n",
    "\n",
    "#fully connected layer\n",
    "model.add(Dense(64, activation=\"relu\"))\n",
    "\n",
    "model.add(Dense(1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compiling\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer=\"adam\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = model.fit(\n",
    "    train_generator,\n",
    "    epochs=10,\n",
    "    validation_data=val_generator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "\n",
    "plt.subplot(2,2,1)\n",
    "plt.plot(model_1.history['loss'], label=\"Training Loss\")\n",
    "plt.plot(model_1.history[\"val_loss\"], label=\"Validation Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Loss Evolution\")\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "plt.plot(model_1.history['accuracy'], label=\"Training Loss\")\n",
    "plt.plot(model_1.history[\"val_accuracy\"], label=\"Validation Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Accuracy Evolution\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = model.evaluate(test_generator)\n",
    "print(f\"Test Accuracy: {evaluation[1] * 100:.2f}%\")\n",
    "\n",
    "evaluation = model.evaluate(train_generator)\n",
    "print(f\"Train Accuracy: {evaluation[1] * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = test_generator.classes\n",
    "y_pred = (model.predict(test_generator) > 0.5).astype(\"int32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "confusion_matrix = confusion_matrix(y_true, y_pred)\n",
    "sns.heatmap(confusion_matrix, annot=True, fmt=\"d\")\n",
    "\n",
    "plt.xlabel(\"Predicted Label\", fontsize=12)\n",
    "plt.xlabel(\"True Label\", fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision, Recall and F1-score of the model\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix.ravel()\n",
    "\n",
    "precision = tp/(tp+fp)\n",
    "recall = tp/(tp+fn)\n",
    "f1_score = (2*precision*recall/(precision+recall))\n",
    "\n",
    "print(\"Recall of the model is {:.2f}\".format(recall))\n",
    "print(\"Precision of the model is {:.2f}\".format(precision))\n",
    "print(\"F1-Score: {}\".format(f1_score))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improve the model\n",
    "\n",
    "## Image Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen_2 = ImageDataGenerator(\n",
    "                        rescale=1./255,\n",
    "                        shear_range=0.2,\n",
    "                        zoom_range=0.2,\n",
    "                        horizontal_flip=True,\n",
    "                        rotation_range=10,\n",
    "                        fill_mode=\"nearest\")\n",
    "\n",
    "val_datagen = ImageDataGenerator(\n",
    "                        rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator_2 = train_datagen_2.flow_from_dataframe(\n",
    "                    train_df,\n",
    "                    x_col=\"image\",\n",
    "                    y_col=\"class\",\n",
    "                    target_size=(150,150),\n",
    "                    batch_size=32,\n",
    "                    class_mode=\"binary\",\n",
    "                    seed=7)\n",
    "\n",
    "val_generator = val_datagen.flow_from_dataframe(\n",
    "                val_df,\n",
    "                x_col=\"image\",\n",
    "                y_col=\"class\",\n",
    "                target_size=(150,150),\n",
    "                batch_size=32,\n",
    "                class_mode=\"binary\",\n",
    "                seed=7)\n",
    "\n",
    "test_generator = val_datagen.flow_from_dataframe(\n",
    "                 df_test,\n",
    "                 x_col=\"image\",\n",
    "                 y_col=\"class\",\n",
    "                 target_size=(150,150),\n",
    "                 batch_size=32,\n",
    "                 class_mode=\"binary\",\n",
    "                 shuffle=False,\n",
    "                 seed=7)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor=\"val_accuracy\", factor=0.1, min_delta=0.0001, patience=1, verbose=1)\n",
    "\n",
    "filepath=\"weights.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor=\"val_accuracy\", verbose=1, save_best_only=True, mode=\"max\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network with Paddings (Extends the area of image in which the CNN acts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(filters=32, kernel_size=(3,3), activation=\"relu\", padding=\"same\", input_shape=(150, 150, 3)))\n",
    "\n",
    "model.add(Conv2D(filters=32, kernel_size=(3,3), activation=\"relu\", padding=\"same\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=(3,3), activation=\"relu\", padding=\"same\"))\n",
    "model.add(Conv2D(filters=64, kernel_size=(3,3), activation=\"relu\", padding=\"same\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation=\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(rate=0.6))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer=Adam(learning_rate=0.0001),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "\n",
    "htuning_model = model.fit(\n",
    "                train_generator_2,\n",
    "                epochs=10,\n",
    "                validation_data=val_generator,\n",
    "                callbacks=[reduce_lr, checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "\n",
    "plt.subplot(2,2,1)\n",
    "plt.plot(htuning_model.history['loss'], label=\"Training Loss\")\n",
    "plt.plot(htuning_model.history[\"val_loss\"], label=\"Validation Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Loss Evolution\")\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "plt.plot(htuning_model.history['accuracy'], label=\"Training Loss\")\n",
    "plt.plot(htuning_model.history[\"val_accuracy\"], label=\"Validation Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Accuracy Evolution\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = model.evaluate(test_generator)\n",
    "print(f\"Test Accuracy: {evaluation[1] * 100:.2f}%\")\n",
    "\n",
    "evaluation = model.evaluate(train_generator)\n",
    "print(f\"Train Accuracy: {evaluation[1] * 100:.2f}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confustion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = test_generator.classes\n",
    "y_pred = (model.predict(test_generator) > 0.5).astype(\"int32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "confusion_matrix = confusion_matrix(y_true, y_pred)\n",
    "sns.heatmap(confusion_matrix, annot=True, fmt=\"d\")\n",
    "\n",
    "plt.xlabel(\"Predicted Label\", fontsize=12)\n",
    "plt.xlabel(\"True Label\", fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn, fp, fn, tp = confusion_matrix.ravel()\n",
    "\n",
    "precision = tp/(tp+fp)\n",
    "recall = tp/(tp+fn)\n",
    "f1_score = (2*precision*recall/(precision+recall))\n",
    "\n",
    "print(\"Recall of the model is {:.2f}\".format(recall))\n",
    "print(\"Precision of the model is {:.2f}\".format(precision))\n",
    "print(\"F1-Score: {}\".format(f1_score))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning (Not really recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import ResNet152V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_base_model = ResNet152V2(input_shape=(150,150,3), include_top=False, weights=\"imagenet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_t1 = Sequential()\n",
    "model_t1.add(resnet_base_model)\n",
    "model_t1.add(Flatten())\n",
    "\n",
    "model_t1.add(Dense(1024, activation=\"relu\"))\n",
    "model_t1.add(BatchNormalization())\n",
    "model_t1.add(Dropout(rate=0.5))\n",
    "\n",
    "model_t1.add(Dense(128, activation=\"relu\"))\n",
    "model_t1.add(BatchNormalization())\n",
    "model_t1.add(Dropout(rate=0.4))\n",
    "\n",
    "model_t1.add(Dense(1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Freeze initial layer of the network\n",
    "resnet_base_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_t1.compile(loss=\"binary_crossentropy\",\n",
    "                 optimizer=Adam(learning_rate=0.001),\n",
    "                 metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_t1_final = model_t1.fit(\n",
    "                 train_generator_2,\n",
    "                 epochs=10,\n",
    "                 validation_data=val_generator,\n",
    "                 callbacks=[reduce_lr, checkpoint])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "\n",
    "plt.subplot(2,2,1)\n",
    "plt.plot(model_t1_final.history['loss'], label=\"Training Loss\")\n",
    "plt.plot(model_t1_final.history[\"val_loss\"], label=\"Validation Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Loss Evolution\")\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "plt.plot(model_t1_final.history['accuracy'], label=\"Training Loss\")\n",
    "plt.plot(model_t1_final.history[\"val_accuracy\"], label=\"Validation Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Accuracy Evolution\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = model_t1.evaluate(test_generator)\n",
    "print(f\"Test Accuracy: {evaluation[1] * 100:.2f}%\")\n",
    "\n",
    "evaluation = model_t1.evaluate(train_generator)\n",
    "print(f\"Train Accuracy: {evaluation[1] * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = test_generator.classes\n",
    "y_pred = (model_t1.predict(test_generator) > 0.5).astype(\"int32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "confusion_matrix = confusion_matrix(y_true, y_pred)\n",
    "sns.heatmap(confusion_matrix, annot=True, fmt=\"d\")\n",
    "\n",
    "plt.xlabel(\"Predicted Label\", fontsize=12)\n",
    "plt.xlabel(\"True Label\", fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn, fp, fn, tp = confusion_matrix.ravel()\n",
    "\n",
    "precision = tp/(tp+fp)\n",
    "recall = tp/(tp+fn)\n",
    "f1_score = (2*precision*recall/(precision+recall))\n",
    "\n",
    "print(\"Recall of the model is {:.2f}\".format(recall))\n",
    "print(\"Precision of the model is {:.2f}\".format(precision))\n",
    "print(\"F1-Score: {}\".format(f1_score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f9940b9e3e9e77e2db53b4b4925ff681ec6a2164a8576b224408351d156394a3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
